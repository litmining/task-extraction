{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Task Extraction\n",
    "**Purpose:**\n",
    "- Extracts cognitive tasks using LLM (GPT-4-mini via OpenRouter)\n",
    "- Validates LLM annotations against original articles\n",
    "- Compares annotations from different sources (LLM, LabelBuddy, NeuroVault)\n",
    "- Exports task combinations to CSV files\n",
    "\n",
    "**Data dependencies:**\n",
    "\n",
    "The notebook works with data stored in:\n",
    "- `data/neurovault_labeled_papers/`\n",
    "- Generated output pickle files include timestamps for versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "notebook_path = os.getcwd()  # Get the current working directory\n",
    "project_root = os.path.dirname(notebook_path)  # Get the parent directory\n",
    "sys.path.append(project_root)\n",
    "os.chdir(project_root)\n",
    "sys.path.append(project_root + '/src')\n",
    "sys.path = list(set(sys.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4e-05\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.lb_annotation_utils import Article\n",
    "from src.llm_utils import system_prompts, estimate_token_cost\n",
    "import re\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=os.getenv(\"OPENROUTER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "**Summary:**\n",
    "1. Load article annotations from LabelBuddy\n",
    "2. Filter annotated articles based on exclusion criteria:\n",
    "    - Resting state studies\n",
    "    - Meta-analyses\n",
    "    - Articles without explicit methods sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MAP_FOLDER = \"data/neurovault_labeled_papers/\"\n",
    "INDEX_FILE = MAP_FOLDER + \"nv_labeled_tasks_pmcid_map.json\"\n",
    "TASK_FILE_PATTERN = MAP_FOLDER + \"nv_labeled_tasks_{chunk_number}.json\"\n",
    "LB_ANNOTATION = os.getenv(\"LB_ANNOTATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = json.load(open(LB_ANNOTATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \n",
      "# Title\n",
      "\n",
      "The role of the hippocampus in generalizing configural relationships\tNo subheaders found in body section\n",
      "\n",
      "Title: \n",
      "# Title\n",
      "\n",
      "A Coordinate-Based Meta-Analysis of Overlaps in Regional Specialization and Functional Connectivity across Subjective Value and Default Mode Networks\tNo subheaders found in body section\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up filter logic\n",
    "# exclusion criteria\n",
    "exclude_types = [\"resting_state\", \"meta-analysis\"]\n",
    "# excluded articles\n",
    "excluded_articles = []\n",
    "# included articles\n",
    "included_articles = []  \n",
    "# extract methods\n",
    "for i in range(len(ann)):\n",
    "    article_obj  = Article(ann[i])\n",
    "    if article_obj.type in exclude_types:\n",
    "        excluded_articles.append({\"pmcid\": article_obj.pmcid, \"exclusion_reason\":{\"type\": article_obj.type}})\n",
    "    elif article_obj.methods == \"No explicit methods section\":\n",
    "        excluded_articles.append({\"pmcid\": article_obj.pmcid, \"exclusion_reason\":{\"methods\": \"No explicit methods section\"}})\n",
    "    else:\n",
    "        included_articles.append(article_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pmcid': 5324609,\n",
       "  'exclusion_reason': {'methods': 'No explicit methods section'}},\n",
       " {'pmcid': 5090046, 'exclusion_reason': {'type': 'resting_state'}},\n",
       " {'pmcid': 10634720, 'exclusion_reason': {'type': 'meta-analysis'}},\n",
       " {'pmcid': 3445793, 'exclusion_reason': {'type': 'meta-analysis'}},\n",
       " {'pmcid': 5243799, 'exclusion_reason': {'type': 'meta-analysis'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<src.lb_annotation_utils.Article at 0x7f42ec3a5a50>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a6b30>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a51b0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a5d80>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a5db0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a6cb0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308bd7880>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308bd7e80>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308bd7910>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308bd6cb0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308bd76a0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308255540>,\n",
       " <src.lb_annotation_utils.Article at 0x7f43082546a0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308254040>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308264c40>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308264be0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a59f0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f4308264bb0>,\n",
       " <src.lb_annotation_utils.Article at 0x7f42ec3a5480>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Task Extraction\n",
    "\n",
    "**Summary:**\n",
    "1. Estimate token cost for LLM task extraction\n",
    "2. Extract both task names and task descriptions using OpenRouter API with GPT-4-mini model\n",
    "    - Export the included articles with LLM annotations as pickle file\n",
    "3. Validates extracted tasks against original article text\n",
    "\n",
    "### 2.1 Estimate token cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5548899999999999\n"
     ]
    }
   ],
   "source": [
    "total_cost = 0\n",
    "for i in range(len(included_articles)):\n",
    "    article_obj  = included_articles[i]\n",
    "    if article_obj.methods!=\"No explicit methods section\":\n",
    "        item = {\"pmcid\": article_obj.pmcid, \"methods\": article_obj.methods}\n",
    "        total_cost += estimate_token_cost(article_obj.methods, \"gpt-4o-mini\")\n",
    "    else:\n",
    "        item = {\"pmcid\": article_obj.pmcid, \"methods\": None}\n",
    "print(total_cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract tasks using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_using_llm(text:str, client:OpenAI, messages:list):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'openai/gpt-4o-mini',\n",
    "        messages = messages,\n",
    "        seed = 42, \n",
    "        response_format = {\"type\": \"json_object\"}\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    # check output format\n",
    "    assert isinstance(output, str), \"Output is not a string\"\n",
    "    assert output.startswith('{') and output.endswith('}'), \"Output is not a valid JSON string\"\n",
    "    return output\n",
    "\n",
    "def generate_messages(text:str, system_prompt:str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{text}\"},\n",
    "    ]\n",
    "    return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Extract cognitive tasks, if not present, extract taskdescriptions\n",
    "for i in range(len(included_articles)):\n",
    "    methods = included_articles[i].methods\n",
    "    output = extract_using_llm(text = methods, client = client, call_message = extract_cognitive_task)\n",
    "    if re.search(r'null', output):\n",
    "        output_description = extract_using_llm(text = methods, client = client, call_message = extract_cognitive_task_description)\n",
    "        included_articles[i].llm_annotations[\"cognitive_task_description\"] = json.loads(output_description)['cognitive_task_description']\n",
    "    else:\n",
    "        included_articles[i].llm_annotations[\"cognitive_task\"] = json.loads(output)[\"cognitive_task\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export included articles with LLM annotations\n",
    "\n",
    "This section exports the included articles with LLM annotations as a pickle file so that the LLM annotations can be re-used for future steps. This is useful for reproducibility across multiple LLM extraction runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cognitive tasks AND task descriptions\n",
    "for i in range(len(included_articles)):\n",
    "#for i in range(1):\n",
    "    methods_text = included_articles[i].methods\n",
    "    # Extract cognitive tasks\n",
    "    for k,v in system_prompts.items():\n",
    "        messages = generate_messages(text = methods_text, system_prompt = v)\n",
    "        output = extract_using_llm(text = methods_text, client = client, messages = messages)\n",
    "        output_dict = json.loads(output)\n",
    "\n",
    "        try: \n",
    "            [(key, value)] = output_dict.items()  \n",
    "            included_articles[i].llm_annotations[k] = value\n",
    "        except ValueError:\n",
    "            print(f\"Expected exactly one key-value pair, but got {len(output_dict)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export included_articles as pickle\n",
    "import pickle as pkl\n",
    "import datetime\n",
    "YYYYMMDD_HHMMSS = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "with open(f'data/{YYYYMMDD_HHMMSS}_included_articles.pkl', 'wb') as f:\n",
    "        pkl.dump(included_articles, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "\n",
    "# Find most recent included_articles file\n",
    "data_dir = Path('data')\n",
    "latest_file = max(data_dir.glob('*_included_articles.pkl'))\n",
    "\n",
    "# Load the file\n",
    "with open(latest_file, 'rb') as f:\n",
    "    included_articles = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Validate LLM annotations\n",
    "This section validates the LLM annotations against the original article text. It checks if the extracted tasks match the original article text and identifies any mismatched tasks.\n",
    "\n",
    "Tasks without an exact match in the original article text are identified and moved to the `Article.llm_annotations` dictionary under the `mismatched_tasks` key.\n",
    "\n",
    "Tasks that are matched to the original article text stay in the `Article.llm_annotations` dictionary under the original key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8318202\n",
      "{'TaskName_SimplePrompt': ['attentive listening', 'word repetition'], 'TaskName_DetailedPrompt': ['attentive listening', 'word repetition'], 'TaskDescription': ['Attentive listening task where participants listened to a sequence of auditory sounds, including words, silence, and noise, while focusing on a fixation cross.', 'Word repetition task where participants listened to words and repeated them aloud following the stimulus presentation.']}\n",
      "\n",
      "\n",
      "9202476\n",
      "{'TaskName_SimplePrompt': ['match-to-sample (MTS) task', 'resting-state fMRI'], 'TaskName_DetailedPrompt': [], 'TaskDescription': ['Subjects performed the match-to-sample (MTS) task of the Cambridge Neuropsychological Test Automated Battery (CANTAB) for visual search and attention, where they had to identify a pattern that matched a previously shown complex figure among multiple peripheral patterns. The task involved varying the number of patterns from two to eight across trials, with performance measured in terms of total correct responses and mean reaction times.'], 'mismatched_tasks': {'mismatched_tasks': {'TaskName_DetailedPrompt': ['match-to-sample (MTS) task of the Cambridge Neuropsychological Test Automated Battery (CANTAB)']}}}\n",
      "\n",
      "\n",
      "4440210\n",
      "{'TaskName_SimplePrompt': ['matching task', 'emotional matching task', 'Faces – Forms', 'IAPS Pictures – Forms'], 'TaskName_DetailedPrompt': ['matching task', 'emotional matching task', 'Faces – Forms', 'IAPS Pictures – Forms'], 'TaskDescription': ['Subjects performed a commonly employed matching task designed to activate the amygdala, where they were shown triplets of geometric shapes (neutral stimuli) and of threatening scenes as well as fearful faces (emotional conditions) presented in alternating blocks of neutral and emotional stimuli.']}\n",
      "\n",
      "\n",
      "6175904\n",
      "{'TaskName_SimplePrompt': ['reminiscence task', 'week discrimination task'], 'TaskName_DetailedPrompt': ['episodic', 'active lifelogging', 'recollective experience', 'week discrimination task', 'remembrance period'], 'TaskDescription': ['Participants viewed images from their own lifelogs while being instructed to relive the associated experiences mentally. After viewing each image for 8 seconds, they indicated whether they remembered the event and how vividly they recalled it using a button box.'], 'mismatched_tasks': {'mismatched_tasks': {'TaskName_DetailedPrompt': ['remembering the event depicted in the picture']}}}\n",
      "\n",
      "\n",
      "8564184\n",
      "{'TaskName_SimplePrompt': [None], 'TaskName_DetailedPrompt': [None], 'TaskDescription': [None]}\n",
      "\n",
      "\n",
      "9454014\n",
      "{'TaskName_SimplePrompt': ['Food Cue Reactivity Task'], 'TaskName_DetailedPrompt': ['Food Cue Reactivity Task'], 'TaskDescription': [\"The Food Cue Reactivity Task involved presenting participants with 44 food images (22 high energy-dense foods and 22 low energy-dense foods) and 44 degraded images to serve as a visual baseline, where participants rated their feelings towards each image as 'liked,' 'disliked,' or 'neutral'.\"]}\n",
      "\n",
      "\n",
      "5607552\n",
      "{'TaskName_SimplePrompt': [], 'TaskName_DetailedPrompt': ['D‐I‐D stimulus sets'], 'TaskDescription': ['Subjects were instructed to listen attentively to a series of distorted, intact, and again distorted sentences (D-I-D stimulus set), and to maintain their gaze on a central fixation cross while avoiding movement during the experiment. After each set, subjects indicated by a button press (yes/no) whether the distorted sentences were easier to understand when presented after the intact sentences.'], 'mismatched_tasks': {'mismatched_tasks': {'TaskName_SimplePrompt': ['D-I-D stimulus set'], 'TaskName_DetailedPrompt': ['listening to distorted sentences', 'listening to intact sentences']}}}\n",
      "\n",
      "\n",
      "5404760\n",
      "{'TaskName_SimplePrompt': ['story-reading phase', 'rating phase'], 'TaskName_DetailedPrompt': ['story-reading phase', 'rating phase'], 'TaskDescription': ['Participants performed the revised Social Norm Processing Task (SNPT-R), which included a story-reading phase where they read stories describing social situations, and a rating phase where they rated the stories on embarrassment and inappropriateness.'], 'mismatched_tasks': {'mismatched_tasks': {'TaskName_SimplePrompt': ['Social Norm Processing Task (SNPT-R)'], 'TaskName_DetailedPrompt': ['Social Norm Processing Task (SNPT-R)']}}}\n",
      "\n",
      "\n",
      "10641579\n",
      "{'TaskName_SimplePrompt': ['Chatroom fMRI task'], 'TaskName_DetailedPrompt': ['Chatroom fMRI task'], 'TaskDescription': [\"Participants completed the Chatroom fMRI task, which was administered during the substudy's second year of neuroimaging data collection.\"]}\n",
      "\n",
      "\n",
      "9308012\n",
      "{'TaskName_SimplePrompt': ['Trust Game', 'Investment Game'], 'TaskName_DetailedPrompt': ['Trust Game task', 'investment game', 'trust game task'], 'TaskDescription': ['Participants performed a Trust Game task where they decided how much money to invest with different partners (a friend, a stranger, and a computer), with their choices and the outcomes influencing future decisions.']}\n",
      "\n",
      "\n",
      "7582181\n",
      "{'TaskName_SimplePrompt': ['LODESTARS'], 'TaskName_DetailedPrompt': ['LODESTARS'], 'TaskDescription': ['Participants completed the LODESTARS inventory, which is a 10-item measure assessing their expectancies of social reward and threat in an imaginary scenario involving a social encounter with unfamiliar peers.']}\n",
      "\n",
      "\n",
      "10597625\n",
      "{'TaskName_SimplePrompt': ['CS-alone trials', 'Expectancy rating task', 'Functional localizer task'], 'TaskName_DetailedPrompt': ['Likert response task', 'functional localizer task', 'CS-alone trials', 'CS–US pairs', 'CS\\u2009+\\u2009US trials', 'CS-alone trials'], 'TaskDescription': ['Participants performed a shock expectancy rating task in which they viewed a greyscale face stimulus and rated their expectancy of receiving an electric shock during the inter-trial interval using a two-response button box.', 'Participants engaged in a functional localizer task, where they viewed neutral face and place images in rapid sequences to localize face processing in the amygdala.'], 'mismatched_tasks': {'mismatched_tasks': {'TaskName_SimplePrompt': ['CS+US trials']}}}\n",
      "\n",
      "\n",
      "6200838\n",
      "{'TaskName_SimplePrompt': ['Affective Stroop Task'], 'TaskName_DetailedPrompt': ['Affective Stroop Task', 'negative-congruent', 'neutral-incongruent', 'negative-blank', 'congruent', 'incongruent', 'blank'], 'TaskDescription': ['The Affective Stroop Task, where participants responded to the number of digits displayed following an emotional stimulus (negative or neutral), categorizing them as congruent or incongruent.']}\n",
      "\n",
      "\n",
      "4961028\n",
      "{'TaskName_SimplePrompt': [None], 'TaskName_DetailedPrompt': [None], 'TaskDescription': [None]}\n",
      "\n",
      "\n",
      "7426775\n",
      "{'TaskName_SimplePrompt': [None], 'TaskName_DetailedPrompt': [None], 'TaskDescription': [None]}\n",
      "\n",
      "\n",
      "7018765\n",
      "{'TaskName_SimplePrompt': ['TEAM task', 's-detection trials'], 'TaskName_DetailedPrompt': ['TEAM task', 's-detection trials'], 'TaskDescription': ['The TEAM task is an event-related design and consists of 17 trials during which participants first see a pattern of colored arrows presented sequentially on the screen for 3 s, twice in a row (totaling 6 s). They are then given 4 s to reconstruct the sequence by pressing colored buttons on a response box.', \"The task includes 's-detection' trials where participants see a random 20-letter string for 6 s and are asked to press a button if the string includes the letter 's'.\"]}\n",
      "\n",
      "\n",
      "2241626\n",
      "{'TaskName_SimplePrompt': ['passive viewing of flashing horizontal checkerboards', 'passive viewing of flashing vertical checkerboards', 'pressing three times the left button according to auditory instruction', 'pressing three times the right button according to auditory instruction', 'silently reading short visual sentences', 'listening to short sentences', 'solving silently visual subtraction problems', 'solving silently auditory subtraction problems'], 'TaskName_DetailedPrompt': ['passive viewing of flashing horizontal checkerboards', 'passive viewing of flashing vertical checkerboards', 'pressing three times the left button with the left thumb button according to visual instructions', 'pressing three times the right button according to visual instruction', 'pressing three times the left button according to auditory instruction', 'pressing three times the right button according to auditory instruction', 'silently reading short visual sentences', 'listening to short sentences', 'solving silently visual subtraction problems', 'solving silently auditory subtraction problems'], 'TaskDescription': ['Passive viewing of flashing horizontal checkerboards', 'Passive viewing of flashing vertical checkerboards', 'Pressing three times the left button with the left thumb according to visual instructions', 'Pressing three times the right button according to visual instruction', 'Pressing three times the left button according to auditory instruction', 'Pressing three times the right button according to auditory instruction', 'Silently reading short visual sentences', 'Listening to short sentences', 'Solving silently visual subtraction problems', 'Solving silently auditory subtraction problems'], 'mismatched_tasks': {'mismatched_tasks': {'TaskName_SimplePrompt': ['pressing three times the left button with the left thumb according to visual instructions', 'pressing three times the right button according to visual instructions']}}}\n",
      "\n",
      "\n",
      "7377905\n",
      "{'TaskName_SimplePrompt': ['social perceptual decision task'], 'TaskName_DetailedPrompt': ['social perceptual decision task', 'perceptual task', 'perceptual part of the task', 'confidence model'], 'TaskDescription': ['Subjects performed a social perceptual decision task where they had to indicate the average direction of motion of moving dots, report their confidence in this decision, and view feedback on the accuracy of group decisions.']}\n",
      "\n",
      "\n",
      "4441475\n",
      "{'TaskName_SimplePrompt': ['Composite Letter Task with Motivation Manipulation'], 'TaskName_DetailedPrompt': ['composite letter task'], 'TaskDescription': [\"Participants completed a modified version of the composite letter task, where they indicated with a button press whether a presented composite letter stimulus contained either a 'T' or an 'H', with the target letters presented in a global or local context, and preceded by motivational images that varied across blocks.\"]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify Article.llm_annotations is from original Article.methods\n",
    "for i in range(len(included_articles)):\n",
    "    print(included_articles[i].pmcid)\n",
    "    methods = included_articles[i].methods.lower()\n",
    "    # match task names in Article.llm_annotations to Article.methods\n",
    "    mismatched_tasks = {'mismatched_tasks': {}}\n",
    "    for k,v in included_articles[i].llm_annotations.items():\n",
    "        if \"TaskName\" in k:\n",
    "            for name in v:\n",
    "                if name is None:\n",
    "                    continue\n",
    "                pattern = r'\\b' + re.escape(name.lower()) + r'\\b'\n",
    "                if re.search(pattern, methods):\n",
    "                    continue\n",
    "                else:\n",
    "                    # add to list of mismatched tasks\n",
    "                    if k not in mismatched_tasks['mismatched_tasks'].keys():\n",
    "                        mismatched_tasks['mismatched_tasks'][k] = []\n",
    "                    mismatched_tasks['mismatched_tasks'][k].append(name)\n",
    "    if len(mismatched_tasks['mismatched_tasks']) > 0:\n",
    "        included_articles[i].llm_annotations['mismatched_tasks'] = mismatched_tasks\n",
    "        # remove mismatched tasks from Article.llm_annotations\n",
    "        for k,v in mismatched_tasks['mismatched_tasks'].items():\n",
    "            included_articles[i].llm_annotations[k] = [task for task in included_articles[i].llm_annotations[k] if task not in v]   \n",
    "    print(included_articles[i].llm_annotations)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare LLM annotations with NeuroVault and LabelBuddy\n",
    "\n",
    "This section combines annotations from multiple sources:\n",
    "- LLM-extracted tasks\n",
    "- LabelBuddy annotations\n",
    "- NeuroVault labeled tasks\n",
    "\n",
    "**Summary:**\n",
    "1. Add NeuroVault task annotations to included articles\n",
    "2. Compare LLM annotations with LLM annotations\n",
    "3. Create a CSV table of task combinations\n",
    "\n",
    "### 3.1 Add NeuroVault task annotations to included articles\n",
    "This section adds the NeuroVault task annotations to the included articles. It works with the pickle file created in the previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['34327333', ['data/neurovault_labeled_papers/nv_labeled_tasks_5.json']]\n",
      "['35720693', ['data/neurovault_labeled_papers/nv_labeled_tasks_0.json']]\n",
      "No NV labeled tasks found for 25994551\n",
      "['30297824', ['data/neurovault_labeled_papers/nv_labeled_tasks_0.json']]\n",
      "No NV labeled tasks found for 34744973\n",
      "['36092648', ['data/neurovault_labeled_papers/nv_labeled_tasks_1.json']]\n",
      "['28948083', ['data/neurovault_labeled_papers/nv_labeled_tasks_11.json']]\n",
      "No NV labeled tasks found for 28441460\n",
      "No NV labeled tasks found for 37922608\n",
      "['35504565', ['data/neurovault_labeled_papers/nv_labeled_tasks_2.json']]\n",
      "No NV labeled tasks found for 33093488\n",
      "No NV labeled tasks found for 37756616\n",
      "No NV labeled tasks found for 30405475\n",
      "['27230218', ['data/neurovault_labeled_papers/nv_labeled_tasks_0.json']]\n",
      "No NV labeled tasks found for 32320123\n",
      "['32116608', ['data/neurovault_labeled_papers/nv_labeled_tasks_11.json']]\n",
      "['17973998', ['data/neurovault_labeled_papers/nv_labeled_tasks_5.json']]\n",
      "['32701449', ['data/neurovault_labeled_papers/nv_labeled_tasks_11.json']]\n",
      "No NV labeled tasks found for 26000735\n",
      "35720693\n",
      "10.3389/fnins.2022.850372\n",
      "['35720693', ['data/neurovault_labeled_papers/nv_labeled_tasks_0.json']]\n",
      "[{'35720693': {'pmcid': '35720693', 'task': {'id': 'trm_4c8a834779883', 'name': 'rest eyes open', 'definition_text': 'Subjects rest passively with their eyes open. Often used as a baseline for comparison for other tasks.'}, 'pmid': '35720693', 'doi': '10.3389/fnins.2022.850372'}}]\n",
      "\n",
      "\n",
      "34327333\n",
      "10.1162/nol_a_00021\n",
      "['34327333', ['data/neurovault_labeled_papers/nv_labeled_tasks_5.json']]\n",
      "[{'34327333': {'pmcid': '34327333', 'task': {'id': 'trm_550b54a8b30f4', 'name': 'language processing fMRI task paradigm', 'definition_text': 'This task was developed by Binder and colleagues (Binder et al. 2011) and uses the E-prime scripts provided by these investigators. The task consists of two runs that each interleave 4 blocks of a story task and 4 blocks of a math task. The lengths of the blocks vary (average of approximately 30 seconds), but the task was designed so that the math task blocks match the length of the story task blocks, with some additional math trials at the end of the task to complete the 3.8 minute run as needed. The story blocks present participants with brief auditory stories (5-9 sentences) adapted from Aesopâ\\x80\\x99s fables, followed by a 2-alternative forcedchoice question that asks participants about the topic of the story. The example provided in the original Binder paper (p. 1466) is â\\x80\\x9cFor example, after a story about an eagle that saves a man who had done him a favor, participants were asked, â\\x80\\x9cWas that about revenge or reciprocity?â\\x80\\x9d The math task also presents trials aurally and requires subjects to complete addition and subtraction problems. The trials present subjects with a series of arithmetic operations (e.g., â\\x80\\x9cfourteen plus twelveâ\\x80\\x9d), followed by â\\x80\\x9cequalsâ\\x80\\x9d and then two choices (e.g., â\\x80\\x9ctwenty-nine or twentysixâ\\x80\\x9d). Participants push a button to select either the first or the second answer. The math task is adaptive to try to maintain a similar level of difficulty across participants. For more details on the task, please see (Binder et al. 2011).\\r\\n\\r\\nReferences for Language Task: Reliable across subjects and robust activation (Binder et al.\\r\\n2011).\\r\\n\\r\\nThis task is included in the Human Connectome Project.\\r\\n\\r\\nhttp://humanconnectome.org/documentation/S500/HCP_S500+MEG2_Release_Reference_Manual.pdf'}, 'pmid': '34327333', 'doi': '10.1162/nol_a_00021'}}]\n"
     ]
    }
   ],
   "source": [
    "def get_nv_labeled_tasks(pmid, index_file = INDEX_FILE,json_file_pattern = TASK_FILE_PATTERN):\n",
    "    nv_labeled_tasks = []\n",
    "    query_pmid = str(pmid)\n",
    "    with open(index_file, \"r\") as f:\n",
    "        indexer = json.load(f)\n",
    "        task_location = indexer[query_pmid]\n",
    "        print([query_pmid, task_location])\n",
    "        for file in task_location:\n",
    "            with open(file, \"r\") as ff:\n",
    "                extracted_tasks = json.load(ff)\n",
    "                for task in extracted_tasks:\n",
    "                    if list(task.keys())[0] == query_pmid:\n",
    "                        nv_labeled_tasks.append(task)\n",
    "    return nv_labeled_tasks\n",
    "# Table\n",
    "for i in range(len(included_articles)):\n",
    "    try:\n",
    "        nv_labeled_tasks = get_nv_labeled_tasks(pmid = included_articles[i].pmid)\n",
    "        id = list(nv_labeled_tasks[0].keys())[0]\n",
    "        included_articles[i].nv_annotations = nv_labeled_tasks[0][id]\n",
    "    except:\n",
    "        print(f\"No NV labeled tasks found for {included_articles[i].pmid}\")\n",
    "        continue\n",
    "\n",
    "print(included_articles[1].pmid)\n",
    "print(included_articles[1].doi)\n",
    "nv_labeled_tasks = get_nv_labeled_tasks(pmcid = included_articles[1].pmid)\n",
    "print(nv_labeled_tasks)\n",
    "print(\"\\n\")\n",
    "print(included_articles[0].pmid)\n",
    "print(included_articles[0].doi)\n",
    "nv_labeled_tasks = get_nv_labeled_tasks(pmcid = included_articles[0].pmid)\n",
    "print(nv_labeled_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare LLM annotations with LabelBuddy annotations\n",
    "\n",
    "This section compares the LLM annotations with the LabelBuddy annotations then creates a table of task combinations between the LLM and LabelBuddy annotations.\n",
    "\n",
    "Each article has a list of LLM annotations containing multiple task names and LabelBuddy annotations containing multiple task names and unsure flags.\n",
    "\n",
    "The process is as follows for each article:\n",
    "1. Match each LLM task name to a LabelBuddy task name\n",
    "2. If the LLM task name is an exact match:\n",
    "    1. Create a tuple of the LLM task name, LabelBuddy task name, and unsure flag. \n",
    "        - Example format: `(LLM_task_name, LabelBuddy_task_name, unsure_flag)`\n",
    "        - Example: `(TaskA, TaskA, 1)`\n",
    "        - `unsure_flag` is 1 if the LabelBuddy task name is in the LabelBuddy unsure annotation, otherwise 0\n",
    "    2. Add the tuple to the `task_combinations` list\n",
    "    3. Remove the  exact match from the list of LabelBuddy task names to match against the next LLM task name.\n",
    "3. Repeat Step 2for all LLM task names\n",
    "4. For the remaining LLM task names, create a tuple of the LLM task name and list of unmatched LabelBuddy task names. Add the tuple to the `task_combinations` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attentive listening', 'attentive listening', 0), ('attentive listening', 'word repetition', 0), ('word repetition', 'attentive listening', 0), ('word repetition', 'word repetition', 0)]\n",
      "None\n",
      "[('matching task', 'matching task', 1), ('matching task', 'emotional matching task', 1), ('emotional matching task', 'matching task', 1), ('emotional matching task', 'emotional matching task', 1), ('Faces – Forms', 'matching task', 1), ('Faces – Forms', 'emotional matching task', 1), ('IAPS Pictures – Forms', 'matching task', 1), ('IAPS Pictures – Forms', 'emotional matching task', 1)]\n",
      "[('reminiscence task', 'personal semantics', 1), ('reminiscence task', 'autobiographical reminiscence', 1), ('week discrimination task', 'personal semantics', 1), ('week discrimination task', 'autobiographical reminiscence', 1)]\n",
      "None\n",
      "[('Food Cue Reactivity Task', 'food cue reactivity task', 1)]\n",
      "None\n",
      "[('story-reading phase', 'Social Norm Processing Task', 0), ('story-reading phase', 'story-reading phase', 1), ('rating phase', 'Social Norm Processing Task', 0), ('rating phase', 'story-reading phase', 1)]\n",
      "[('Chatroom fMRI task', 'Chatroom task', 1), ('Chatroom fMRI task', 'social evaluation', 1), ('Chatroom fMRI task', 'feedback task', 1)]\n",
      "[('Trust Game', 'trust game', 0), ('Investment Game', 'trust game', 0)]\n",
      "None\n",
      "[('CS-alone trials', 'functional localizer task', 0), ('CS-alone trials', 'Likert response task', 1), ('CS-alone trials', 'expectancy rating task', 1), ('CS-alone trials', 'electrical stimulation', 0), ('Expectancy rating task', 'functional localizer task', 0), ('Expectancy rating task', 'Likert response task', 1), ('Expectancy rating task', 'expectancy rating task', 1), ('Expectancy rating task', 'electrical stimulation', 0), ('Functional localizer task', 'functional localizer task', 0), ('Functional localizer task', 'Likert response task', 1), ('Functional localizer task', 'expectancy rating task', 1), ('Functional localizer task', 'electrical stimulation', 0)]\n",
      "[('Affective Stroop Task', 'Stroop task', 0), ('Affective Stroop Task', 'affective number Stroop task', 1)]\n",
      "None\n",
      "None\n",
      "[('TEAM task', 'TEAM task', 1), ('s-detection trials', 'TEAM task', 1)]\n",
      "[('passive viewing of flashing horizontal checkerboards', 'vertical checkerboards', 0), ('passive viewing of flashing horizontal checkerboards', 'horizontal checkerboards', 0), ('passive viewing of flashing horizontal checkerboards', 'mental calculation task', 1), ('passive viewing of flashing horizontal checkerboards', 'language comprehension task ', 1), ('passive viewing of flashing horizontal checkerboards', 'auditory calculations', 1), ('passive viewing of flashing horizontal checkerboards', 'visual calculations', 1), ('passive viewing of flashing vertical checkerboards', 'vertical checkerboards', 0), ('passive viewing of flashing vertical checkerboards', 'horizontal checkerboards', 0), ('passive viewing of flashing vertical checkerboards', 'mental calculation task', 1), ('passive viewing of flashing vertical checkerboards', 'language comprehension task ', 1), ('passive viewing of flashing vertical checkerboards', 'auditory calculations', 1), ('passive viewing of flashing vertical checkerboards', 'visual calculations', 1), ('pressing three times the left button according to auditory instruction', 'vertical checkerboards', 0), ('pressing three times the left button according to auditory instruction', 'horizontal checkerboards', 0), ('pressing three times the left button according to auditory instruction', 'mental calculation task', 1), ('pressing three times the left button according to auditory instruction', 'language comprehension task ', 1), ('pressing three times the left button according to auditory instruction', 'auditory calculations', 1), ('pressing three times the left button according to auditory instruction', 'visual calculations', 1), ('pressing three times the right button according to auditory instruction', 'vertical checkerboards', 0), ('pressing three times the right button according to auditory instruction', 'horizontal checkerboards', 0), ('pressing three times the right button according to auditory instruction', 'mental calculation task', 1), ('pressing three times the right button according to auditory instruction', 'language comprehension task ', 1), ('pressing three times the right button according to auditory instruction', 'auditory calculations', 1), ('pressing three times the right button according to auditory instruction', 'visual calculations', 1), ('silently reading short visual sentences', 'vertical checkerboards', 0), ('silently reading short visual sentences', 'horizontal checkerboards', 0), ('silently reading short visual sentences', 'mental calculation task', 1), ('silently reading short visual sentences', 'language comprehension task ', 1), ('silently reading short visual sentences', 'auditory calculations', 1), ('silently reading short visual sentences', 'visual calculations', 1), ('listening to short sentences', 'vertical checkerboards', 0), ('listening to short sentences', 'horizontal checkerboards', 0), ('listening to short sentences', 'mental calculation task', 1), ('listening to short sentences', 'language comprehension task ', 1), ('listening to short sentences', 'auditory calculations', 1), ('listening to short sentences', 'visual calculations', 1), ('solving silently visual subtraction problems', 'vertical checkerboards', 0), ('solving silently visual subtraction problems', 'horizontal checkerboards', 0), ('solving silently visual subtraction problems', 'mental calculation task', 1), ('solving silently visual subtraction problems', 'language comprehension task ', 1), ('solving silently visual subtraction problems', 'auditory calculations', 1), ('solving silently visual subtraction problems', 'visual calculations', 1), ('solving silently auditory subtraction problems', 'vertical checkerboards', 0), ('solving silently auditory subtraction problems', 'horizontal checkerboards', 0), ('solving silently auditory subtraction problems', 'mental calculation task', 1), ('solving silently auditory subtraction problems', 'language comprehension task ', 1), ('solving silently auditory subtraction problems', 'auditory calculations', 1), ('solving silently auditory subtraction problems', 'visual calculations', 1)]\n",
      "[('social perceptual decision task', 'social perceptual decision task', 1)]\n",
      "[('Composite Letter Task with Motivation Manipulation', 'composite letter task', 1)]\n"
     ]
    }
   ],
   "source": [
    "# create table of cognitive tasks and descriptions\n",
    "def get_value(obj, *keys, default=None):\n",
    "    \"\"\" get nested dictionary values.\"\"\"\n",
    "    try:\n",
    "        result = obj\n",
    "        for key in keys:\n",
    "            result = result[key]\n",
    "        return result\n",
    "    except (KeyError, TypeError):\n",
    "        return default\n",
    "    \n",
    "def get_task_data(article):\n",
    "    \"\"\"Extract task data from a single article.\"\"\"\n",
    "    return {\n",
    "        'pmcid': article.pmcid,\n",
    "        'llm_task': get_value(article.llm_annotations, 'TaskName_SimplePrompt'),\n",
    "        'lb_task': get_value(article.lb_annotations, 'annotations', 'TaskName'),\n",
    "        'lb_unsure': get_value(article.lb_annotations, 'annotations', 'Unsure')\n",
    "    }\n",
    "# columns: pmcid, llm_task, lb_task, lb_unsure\n",
    "for i in range(len(included_articles)):\n",
    "    pmcid = included_articles[i].pmcid\n",
    "    llm_task = get_value(included_articles[i].llm_annotations, 'TaskName_SimplePrompt')\n",
    "    lb_task = get_value(included_articles[i].lb_annotations, 'annotations', 'TaskName')\n",
    "    if lb_task is not None:\n",
    "        lb_task = [task[0] for task in lb_task]\n",
    "    lb_unsure = get_value(included_articles[i].lb_annotations, 'annotations', 'Unsure')\n",
    "    if lb_unsure is not None:\n",
    "        lb_unsure = [task[0] for task in lb_unsure]\n",
    "\n",
    "\n",
    "    # create tuple of lb_task and lb_unsure (task_name, is_unsure_flag)\n",
    "    if lb_task is not None and lb_unsure is not None:\n",
    "        lb_task = [(task, 1 if task in lb_unsure else 0) for task in lb_task]\n",
    "    if lb_task is not None and lb_unsure is None:\n",
    "        lb_task = [(task, 0) for task in lb_task]\n",
    "    # create combination of llm_task and lb_task as tuple using itertools\n",
    "    from itertools import product\n",
    "    if llm_task is not None and lb_task is not None:\n",
    "        llm_lb_combinations = list(product(llm_task, lb_task))\n",
    "    else:\n",
    "        llm_lb_combinations = None\n",
    "    #print(llm_lb_combinations)\n",
    "    if llm_lb_combinations is not None: \n",
    "        # flatten nested tuple\n",
    "        for i in range(len(llm_lb_combinations)):\n",
    "            try:\n",
    "                combination = llm_lb_combinations[i] \n",
    "                lb_task, lb_unsure_flag = combination[1]\n",
    "                llm_task = combination[0]\n",
    "                llm_lb_combinations[i] = (llm_task, lb_task, lb_unsure_flag)\n",
    "            except ValueError:\n",
    "                print(combination)\n",
    "    print(llm_lb_combinations)\n",
    "\n",
    "# lb_unsure: lb_annotation label is unsure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table of cognitive tasks and descriptions\n",
    "    \n",
    "def get_task_data(article, llm_task_key = 'TaskName_SimplePrompt'):\n",
    "    \"\"\"Extract task data from a single article.\"\"\"\n",
    "\n",
    "    def get_value(obj, *keys, default=None):\n",
    "        \"\"\" get nested dictionary values.\"\"\"\n",
    "        try:\n",
    "            result = obj\n",
    "            for key in keys:\n",
    "                result = result[key]\n",
    "            return result\n",
    "        except (KeyError, TypeError):\n",
    "            return default\n",
    "\n",
    "    return {\n",
    "        'pmcid': article.pmccid,\n",
    "        'llm_task': get_value(article.llm_annotations, llm_task_key),\n",
    "        'lb_task': get_value(article.lb_annotations, 'annotations', 'TaskName'),\n",
    "        'lb_unsure': get_value(article.lb_annotations, 'annotations', 'Unsure')\n",
    "    }\n",
    "\n",
    "def process_lb_tasks(lb_task, lb_unsure):\n",
    "    \"\"\"Process labelbox tasks and unsure flags into tuples.\"\"\"\n",
    "    if lb_task is None:\n",
    "        return None\n",
    "        \n",
    "    # Extract first elements from task lists\n",
    "    lb_task = [task[0] for task in lb_task] if lb_task else None\n",
    "    lb_unsure = [task[0] for task in lb_unsure] if lb_unsure else None\n",
    "    \n",
    "    # Create tuples with unsure flags\n",
    "    if lb_task and lb_unsure:\n",
    "        return [(task, 1 if task in lb_unsure else 0) for task in lb_task]\n",
    "    elif lb_task:\n",
    "        return [(task, 0) for task in lb_task]\n",
    "    return None\n",
    "\n",
    "def create_task_combinations(llm_task, lb_task):\n",
    "    \"\"\"Create combinations of LLM and labelbox tasks.\"\"\"\n",
    "    if not (llm_task and lb_task):\n",
    "        return None\n",
    "        \n",
    "    combinations = list(product(llm_task, lb_task))\n",
    "    return [\n",
    "        (llm, lb_task, lb_unsure_flag)\n",
    "        for llm, (lb_task, lb_unsure_flag) in combinations\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates a table of task combinations between the LLM and LabelBuddy annotations from the included articles. Each table contains the LLM output from a single LLM extraction run using a unique system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Main processing loop\n",
    "\n",
    "for llm_task_key in ['TaskName_SimplePrompt', 'TaskName_DetailedPrompt']:\n",
    "    task_combinations = []\n",
    "    for article in included_articles:\n",
    "        # Get raw data\n",
    "        data = get_task_data(article, llm_task_key = llm_task_key)\n",
    "        df = pd.DataFrame()\n",
    "        # Process labelbox tasks\n",
    "        processed_lb_tasks = process_lb_tasks(data['lb_task'], data['lb_unsure'])\n",
    "        \n",
    "        # Create combinations\n",
    "        combinations = create_task_combinations(data['llm_task'], processed_lb_tasks)\n",
    "        if combinations is not None:\n",
    "            # transpose combinations\n",
    "            combinations = list(zip(*combinations))\n",
    "            llm_task, lb_task, lb_unsure_flag = combinations\n",
    "            #print(llm_task)\n",
    "            \n",
    "            task_combinations.append({\n",
    "                'pmcid': [data['pmcid']]*len(llm_task),\n",
    "                'llm_task': list(llm_task),\n",
    "                'lb_task': list(lb_task),\n",
    "                'lb_unsure_flag': list(lb_unsure_flag),\n",
    "            })\n",
    "    # merge task_combinations into one dataframe\n",
    "    \n",
    "    df = pd.concat([pd.DataFrame(task) for task in task_combinations], ignore_index=True)\n",
    "    # get YYYYMMDD_HHMMSS from latest file name\n",
    "    YYYYMMDD = latest_file.stem.split('_')[0]\n",
    "    HHMMSS = latest_file.stem.split('_')[1]\n",
    "    YYYYMMDD_HHMMSS = f\"{YYYYMMDD}_{HHMMSS}\"\n",
    "    df.to_csv(f'data/{YYYYMMDD_HHMMSS}_task_combinations_{llm_task_key}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section prints out the LLM, NeuroVault, and LabelBuddy annotations for each article. This is useful for manual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(included_articles)):\n",
    "    print(included_articles[i].pmcid)\n",
    "    print(i)\n",
    "\n",
    "    # Compare LLM, NV, and LB annotations for cognitive task names\n",
    "    print(\"Cognitive task names:\")\n",
    "    if \"cognitive_task\" in included_articles[i].llm_annotations.keys():\n",
    "        print(\"LLM:\\t\", included_articles[i].llm_annotations['cognitive_task'])\n",
    "    if \"task\" in included_articles[i].nv_annotations.keys():\n",
    "        print(\"NV:\\t\", included_articles[i].nv_annotations['task']['name'])\n",
    "    if \"TaskName\" in included_articles[i].lb_annotations['annotations'].keys():\n",
    "        print(\"LB:\\t\", included_articles[i].lb_annotations['annotations']['TaskName'])\n",
    "    if \"Unsure\" in included_articles[i].lb_annotations['annotations'].keys():\n",
    "        print(\"LB_unsure:\\t\", included_articles[i].lb_annotations['annotations']['Unsure'])\n",
    "    \n",
    "    # Compare LLM and NV annotations for cognitive task descriptions\n",
    "    print(\"\\nCognitive task descriptions:\")\n",
    "    if \"cognitive_task_description\" in included_articles[i].llm_annotations.keys():\n",
    "        print(\"LLM description:\\t\", included_articles[i].llm_annotations['cognitive_task_description'])\n",
    "    #if \"definition_text\" in included_articles[i].nv_annotations.keys():\n",
    "    if \"task\" in included_articles[i].nv_annotations.keys():\n",
    "        definition_text = \"\".join(included_articles[i].nv_annotations['task']['definition_text']).replace(\"\\n\", \"\")\n",
    "        print(\"NV description:\\t\", definition_text)\n",
    "    if \"TaskDescription\" in included_articles[i].lb_annotations['annotations'].keys():\n",
    "        lb_description = [description[0].replace(\"\\n\", \"\") for description in included_articles[i].lb_annotations['annotations']['TaskDescription']]\n",
    "        print(\"LB description:\\t\", lb_description)\n",
    "        \n",
    "    print(\"\\n--------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to scripts/3_llm_extract.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "jupyter nbconvert --to html 3_llm_extract.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Integration with Cognitive Atlas via Jina AI endpoint\n",
    "\n",
    "This section integrates with the Cognitive Atlas via a Jina AI endpoint. It uses the Jina AI endpoint to get the task description from the Cognitive Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "JINA_AI_ENDPOINT = \"https://r.jina.ai/\"\n",
    "cognitive_atlas_url = 'https://www.cognitiveatlas.org'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cognitive_atlas_task(task_url):\n",
    "    url = JINA_AI_ENDPOINT + cognitive_atlas_url + f\"{task_url}\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# test\n",
    "get_cognitive_atlas_task(\"/task/id/trm_4f244f46ebf58\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
